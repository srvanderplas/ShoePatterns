
@article{fowlkesStatisticalModelsGeneration2018,
	title = {Statistical models for the generation and interpretation of shoeprint evidence},
	url = {https://www.nist.gov/sites/default/files/documents/2016/12/05/statistical_models_for_the_generation_and_interpretation_of_shoeprint_evidence.pdf},
	author = {Fowlkes, Charless},
	date = {2018-04-06},
	note = {00000},
	keywords = {⛔ No {DOI} found},
}

@article{kongCrossDomainForensicShoeprint2017,
	title = {Cross-Domain Forensic Shoeprint Matching},
	abstract = {We investigate the problem of automatically determining what type of shoe left an impression found at a crime scene. This recognition problem is made difﬁcult by the variability in types of crime scene evidence (ranging from traces of dust or oil on hard surfaces to impressions made in soil) and the lack of comprehensive databases of shoe outsole tread patterns. We ﬁnd that mid-level features extracted by pre-trained convolutional neural nets are surprisingly effective descriptors for these specialized domains. However, the choice of similarity measure for matching exemplars to a query image is essential to good performance. For matching multi-channel deep features, we propose the use of multi-channel normalized cross-correlation and analyze its effectiveness. Finally, we introduce a discriminatively trained variant and ﬁne-tune our system end-to-end, obtaining state-of-the-art performance.},
	pages = {17},
	journaltitle = {British Machine Vision Conference},
	author = {Kong, Bailey and Supancic, James and Ramanan, Deva and Fowlkes, Charless},
	date = {2017},
	langid = {english},
	note = {00008},
	keywords = {⛔ No {DOI} found},
	file = {Kong - Cross-Domain Forensic Shoeprint Matching.pdf:/home/susan/Nextcloud/Zotero/storage/GHZ5R8U6/Kong - Cross-Domain Forensic Shoeprint Matching.pdf:application/pdf},
}

@article{kongCrossDomainImageMatching2019,
	title = {Cross-Domain Image Matching with Deep Feature Maps},
	issn = {0920-5691, 1573-1405},
	url = {http://arxiv.org/abs/1804.02367},
	doi = {10/ggxkb8},
	abstract = {We investigate the problem of automatically determining what type of shoe left an impression found at a crime scene. This recognition problem is made difficult by the variability in types of crime scene evidence (ranging from traces of dust or oil on hard surfaces to impressions made in soil) and the lack of comprehensive databases of shoe outsole tread patterns. We find that mid-level features extracted by pre-trained convolutional neural nets are surprisingly effective descriptors for this specialized domains. However, the choice of similarity measure for matching exemplars to a query image is essential to good performance. For matching multi-channel deep features, we propose the use of multi-channel normalized cross-correlation and analyze its effectiveness. Our proposed metric significantly improves performance in matching crime scene shoeprints to laboratory test impressions. We also show its effectiveness in other cross-domain image retrieval problems: matching facade images to segmentation labels and aerial photos to map images. Finally, we introduce a discriminatively trained variant and fine-tune our system through our proposed metric, obtaining state-of-the-art performance.},
	journaltitle = {International Journal of Computer Vision},
	author = {Kong, Bailey and Supancic, James and Ramanan, Deva and Fowlkes, Charless C.},
	urldate = {2019-03-14},
	date = {2019-01-04},
	eprinttype = {arxiv},
	eprint = {1804.02367},
	note = {00001 },
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Cross-domain image matching, Normalized cross-correlation, Similarity metric},
	file = {arXiv.org Snapshot:/home/susan/Nextcloud/Zotero/storage/64GB9QTA/1804.html:text/html;Kong et al_2019_Cross-Domain Image Matching with Deep Feature Maps.pdf:/home/susan/Nextcloud/Zotero/storage/E4UGFL5U/Kong et al_2019_Cross-Domain Image Matching with Deep Feature Maps.pdf:application/pdf;Submitted Version:/home/susan/Nextcloud/Zotero/storage/7HNXBGTU/Kong et al. - 2019 - Cross-Domain Image Matching with Deep Feature Maps.pdf:application/pdf},
}
